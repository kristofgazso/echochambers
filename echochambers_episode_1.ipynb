{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e279e3dd",
   "metadata": {},
   "source": [
    "# Echochambers Episode 1: The Optimization Menace\n",
    "\n",
    "Created by Kristof Gazso (@kristofgazso)\n",
    "\n",
    "Echochambers are the usually most vilifying and useless form of communities. Little critical thinking is invested into the ideas circulated beyond the justification of already held beliefs, and any conflicting ideas are usually ridiculed or just ignored at worst. Platforms like Twitter and Reddit provide safe haven for a many echochambers on the internet as they make it incredibly easy to become intrenched in certain communities (like subreddits or twitter sub-cultures).\n",
    "\n",
    "Here I attempt to model some of the behaviours of echochambers: simply at first, then building up complexity. I will see if I can find some non-intuitive and interesting conclusions while doing so.\n",
    "\n",
    "A lot of the initial modelling will come from Frank Witte's ECON0055 Economics of Science lectures\n",
    "\n",
    "NOTE: the models created here are just that: models. There is a lot of oversimplification that happens when creating them and they shouldn't be treated as 100% accurate in predicting human behaviour."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a2e613",
   "metadata": {},
   "source": [
    "To run the cells, simply press `SHIFT+ENTER` on them. Feel free to play around with any initial parameters in the cells and rerun them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77936df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from IPython import display\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58746306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to define some functions that will allow us to repesent the data that we come across during modelling\n",
    "\n",
    "def plot_beliefs_and_evidence(beliefs, evidence):\n",
    "    fig, axs = plt.subplots(2)\n",
    "    axs[0].imshow(beliefs.detach(), cmap=\"bwr\", norm=matplotlib.colors.Normalize(-1, 1, True))\n",
    "    axs[0].set_title(\"Agent beliefs\", fontsize=\"x-large\")\n",
    "    axs[0].set_xlabel(\"Agent number\")\n",
    "    axs[0].set_xticks(range(beliefs.shape[1]))\n",
    "    axs[0].tick_params(axis=\"y\", which=\"both\", left=False, right=False, labelleft=False)\n",
    "    axs[1].imshow(evidence.detach(), cmap=\"bwr\", norm=matplotlib.colors.Normalize(-1, 1, True))\n",
    "    axs[1].set_title(\"Agent evidence\", fontsize=\"x-large\")\n",
    "    axs[1].set_xlabel(\"Agent number\")\n",
    "    axs[1].set_xticks(range(evidence.shape[1]))\n",
    "    axs[1].tick_params(axis=\"y\", which=\"both\", left=False, right=False, labelleft=False)\n",
    "    plt.show()\n",
    "\n",
    "def show_adjacency_matrix(listen_factor):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(listen_factor.detach(), cmap=\"PRGn\", norm=matplotlib.colors.Normalize(-1, 1, True))\n",
    "    ax.set_title(\"Agent beliefs\", fontsize=\"x-large\")\n",
    "    ax.set_xlabel(\"Listener\")\n",
    "    ax.set_ylabel(\"Listening\")\n",
    "    _ = ax.set_xticks(torch.arange(0, listen_factor.shape[1], listen_factor.shape[1]//10))\n",
    "    _ = ax.set_yticks(torch.arange(0, listen_factor.shape[0], listen_factor.shape[1]//10))\n",
    "    \n",
    "def show_adjacency_graph(listen_factor):\n",
    "    G2 = nx.from_numpy_array(listen_factor.numpy())\n",
    "    _ = nx.draw_circular(G2, arrows=True)\n",
    "    _ = plt.axis('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bf3d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "num_agents = 10\n",
    "scepticism = torch.Tensor([2]) # form 0 to inf, the higher the number the more sceptic they are\n",
    "\n",
    "beliefs = torch.divide(torch.randn(1, num_agents), 10).requires_grad_()\n",
    "evidence = torch.divide(torch.randn(1, num_agents), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db72609d",
   "metadata": {},
   "source": [
    "Initially, we will model agents in a relatively simple way. Each of the ten agents will have a fixed evidence variable, which will be relatively small random value (0 mean and 0.1 SD). Then, they will have another attribute called their belief, which is another small random value (0 mean and 0.1 SD) that represents which way on a particular issue they believe in. In this simulation, the issue will just have two opposing viewpoints, and will be a continuous number from -inf to +inf representing which side and how strong they believe in the viewpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b36b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Initial beliefs and evidence:\")\n",
    "plot_beliefs_and_evidence(beliefs, evidence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff7a018",
   "metadata": {},
   "source": [
    "Finally, we get them to maximize their utility. Their utility will be defined simply as the product of their beliefs and the evidence for that belief, however we will subtract from this a value that is proportional to their belief to show that they might have some scepticism. It makes sense that for them to maximize their utility, they will have to change their beliefs to fit the little evidence that they have by making it the same sign. After each \"timeslot\", they will look at which way they should change their beliefs to increase their utility and make a small step towards it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f613e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(50000):\n",
    "\n",
    "    if t % 10000 == 9999:\n",
    "        display.clear_output(wait=True)\n",
    "        print(t, \"steps\")\n",
    "        plot_beliefs_and_evidence(beliefs, evidence)\n",
    "        print(\"Average agent utility:\", utility.mean().item())\n",
    "\n",
    "    utility = beliefs * evidence - scepticism/2*beliefs.square()\n",
    "    \n",
    "    # Find gradients for the beliefs to increase utility\n",
    "    utility.backward(torch.ones_like(utility))\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    with torch.no_grad():\n",
    "        beliefs += learning_rate * beliefs.grad\n",
    "        beliefs.grad.zero_()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf0a564",
   "metadata": {},
   "source": [
    "As we can see, they change their beliefs in order to increase their utility, settling when their beliefs are close to their evidence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c6aa5f",
   "metadata": {},
   "source": [
    "# Listening to Peer beliefs\n",
    "\n",
    "So far, in order to decide on their beliefs, agents only had their own evidence to work with. This is too simplistic. In real life, a lot of our beliefs are influenced by the people we listen to and interact with, and so I will attempt to model this interaction. In this model, each agent will be allocated a fixed 0 or 1 pointing to every other agent to signify if they \"listen\" to this other agent. If they do, then part of their utility will be derived from having similar beliefs to them. This will be a directed graph, i.e. just because agent X listens to agent Y, it doesn't mean that agent Y will necessarily listen to agent X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e5614b",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "num_agents = 10\n",
    "scepticism = torch.Tensor([2]) # form 0 to inf, the higher the number the more sceptic they are\n",
    "peer_multiplier = torch.Tensor([1]) # how much agents value the people they listen to\n",
    "listen_chance = 0.35 # the chance of agent x listening to agent y, the higher the number the more connections\n",
    "\n",
    "beliefs = torch.divide(torch.randn(1, num_agents), 10).requires_grad_()\n",
    "evidence = torch.divide(torch.randn(1, num_agents), 10)\n",
    "listen_factor = (torch.rand(size=(num_agents,num_agents)) < listen_chance).int() # generate either 0 or 1\n",
    "listen_factor = listen_factor.fill_diagonal_(0) # they will listen to themselves, but that is already included\n",
    "\n",
    "show_adjacency_matrix(listen_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff770310",
   "metadata": {},
   "source": [
    "On the adjacency matrix above, the x axis represenets if the agent is \"listening\", and the y axis represents if that agent is being \"listened to\". If (4, 2) is green for example, it would mean that 2 listens to 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fa024a",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_adjacency_graph(listen_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0f5fd3",
   "metadata": {},
   "source": [
    "Above might be a clearer representation of who listens to who"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aee3e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(20000):\n",
    "\n",
    "    if t % 1000 == 999:\n",
    "        display.clear_output(wait=True)\n",
    "        print(t, \"steps\")\n",
    "        plot_beliefs_and_evidence(beliefs, evidence)\n",
    "        print(\"Average agent utility:\", utility.mean().item())\n",
    "        \n",
    "    self_utility = beliefs * evidence\n",
    "    scepticism_utility = scepticism/2*beliefs.square()\n",
    "    peer_utility = peer_multiplier * torch.sum((listen_factor * beliefs.t()), 0) * beliefs\n",
    "\n",
    "    utility = self_utility - scepticism_utility + peer_utility\n",
    "    \n",
    "    # Find gradients for the beliefs to increase utility\n",
    "    utility.backward(torch.ones_like(utility))\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    with torch.no_grad():\n",
    "        beliefs += learning_rate * beliefs.grad\n",
    "        beliefs.grad.zero_()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de1b97a",
   "metadata": {},
   "source": [
    "Its wroth resetting and rerunning this simulation several times. You will find most of the times that all of the agents end up having the same beliefs, but it is quite random whether that will be red (positive) or blue (negative)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc100ca",
   "metadata": {},
   "source": [
    "# Changing avaiable evidence\n",
    "\n",
    "What happens if we also let agents \"change\" the evidence that they have available for themselves. This could represent selectively accepting or rejecting evidence. In order to make it so that this effect doesn't make the model explode too quickly, we also include an `evidence_pressure` parameter that quadratically punishes agents for having their subjective evidence being far away from the real evidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2654e6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "beliefs = torch.divide(torch.randn(1, num_agents), 10).requires_grad_()\n",
    "evidence = torch.divide(torch.randn(1, num_agents), 10).requires_grad_()\n",
    "listen_chance = 0.35 # the chance of agent x listening to agent y, the higher the number the more connections\n",
    "listen_factor = (torch.rand(size=(num_agents,num_agents)) < listen_chance).int() # generate either 0 or 1\n",
    "listen_factor = listen_factor.fill_diagonal_(0) # they will listen to themselves, but that doesn't need to be part of the listen_factor\n",
    "\n",
    "real_evidence = 0\n",
    "evidence_pressure = torch.Tensor([4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ff1583",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_adjacency_matrix(listen_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379f66af",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_adjacency_graph(listen_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a5e01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "for t in range(2000):\n",
    "    if t % 100 == 99:\n",
    "        display.clear_output(wait=True)\n",
    "        print(t, \"steps\")\n",
    "        plot_beliefs_and_evidence(beliefs, evidence)\n",
    "        print(\"Average agent utility:\", utility.mean().item())\n",
    "\n",
    "    utility = beliefs * evidence - scepticism/2*beliefs.square() + peer_multiplier * torch.sum((listen_factor * beliefs.t()), 0) * beliefs - evidence_pressure * (evidence - real_evidence).square()\n",
    "    \n",
    "    # Find gradients for the beliefs to increase utility\n",
    "    utility.backward(torch.ones_like(utility))\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    with torch.no_grad():\n",
    "        beliefs += learning_rate * beliefs.grad\n",
    "        evidence += learning_rate * evidence.grad\n",
    "        beliefs.grad.zero_()\n",
    "        evidence.grad.zero_()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc7f4c3",
   "metadata": {},
   "source": [
    "Interestingly we see that agents usually end up completely radicalized in their beliefs before the \"evidence\" that they have starts to drastically change."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df9527c",
   "metadata": {},
   "source": [
    "# Allowing them to change who they listen to\n",
    "\n",
    "Until now we made it so that the agents an agent listened to was fixed and unchangeable. This is quite an unrealistic scenario as many people would choose to listen more to people to they agree with and less to people they disagree with. Furthermore, we can allow agents to have negative `listen_factor` values, which would mean that they would believe the opposite of what another agent believes. We will start out with a small and normally distributed `listen_factor`.\n",
    "\n",
    "NOTE: for this model we have disabled the ability for agents to change their evidence. We will reenable this in the next model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013dfc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_agents = 10\n",
    "scepticism = torch.Tensor([2]) # form 0 to inf, the higher the number the more sceptic they are\n",
    "peer_multiplier = torch.Tensor([2.5]) # how much agents value the people they listen to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbe9945",
   "metadata": {},
   "outputs": [],
   "source": [
    "beliefs = torch.divide(torch.randn(1, num_agents), 10).requires_grad_()\n",
    "evidence = torch.divide(torch.randn(1, num_agents), 50)\n",
    "listen_factor = torch.randn((num_agents, num_agents)).divide(10).fill_diagonal_(0).requires_grad_() # generate either 0 or 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0486c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "show_adjacency_matrix(listen_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dfb169",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "for t in range(15000):\n",
    "    if t % 1000 == 999:\n",
    "        display.clear_output(wait=True)\n",
    "        print(t, \"steps\")\n",
    "        plot_beliefs_and_evidence(beliefs, evidence)\n",
    "        show_adjacency_matrix(listen_factor)\n",
    "        print(\"Average agent utility:\", utility.mean().item())\n",
    "\n",
    "    self_utility = beliefs * evidence\n",
    "    scepticism_utility = scepticism/2*beliefs.square()\n",
    "    peer_utility = peer_multiplier * torch.sum((listen_factor * beliefs.t()), 0) * beliefs\n",
    "    utility =  self_utility - scepticism_utility + peer_utility\n",
    "    \n",
    "    # Find gradients for the beliefs to increase utility\n",
    "    utility.backward(torch.ones_like(utility))\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    with torch.no_grad():\n",
    "        beliefs += learning_rate * beliefs.grad\n",
    "        beliefs.grad.zero_()\n",
    "        beliefs = beliefs.clamp_(-1, 1)\n",
    "        \n",
    "        listen_factor += learning_rate * listen_factor.grad\n",
    "        listen_factor.grad.zero_()\n",
    "        listen_factor = listen_factor.fill_diagonal_(0)\n",
    "        listen_factor = listen_factor.clamp_(-1, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f09bfc3",
   "metadata": {},
   "source": [
    "We see that agents have developed a complete echochamber of ideas. They end up with a very peculiar `listen_factor`. They end up listening completely to agents that have the same beliefs as them and end up wanting to believe the opposite of agents that have the opposite beliefs as them. Basically, in this model, they usually end up forming two 'bubbles' where they listen completely to agents that are in the same bubbles and want to disagree with agents who are in the opposite bubble.\n",
    "\n",
    "Also worth noting that agents seem to decide their beliefs slightly before they make major changes to who they listen to."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae52d56c",
   "metadata": {},
   "source": [
    "# Allowing them to selectively include new evidence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2269d5",
   "metadata": {},
   "source": [
    "In this final model of Echochambers Episode 1, we simply allow the agents to change their evidence again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0798702",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "num_agents = 10\n",
    "scepticism = torch.Tensor([2]) # form 0 to inf, the higher the number the more sceptic they are\n",
    "peer_multiplier = torch.Tensor([2.5]) # how much agents value the people they listen to\n",
    "evidence_pressure = torch.Tensor([2]) # lets make this have quite a harsh penalty\n",
    "\n",
    "beliefs = torch.divide(torch.randn(1, num_agents), 10).requires_grad_()\n",
    "evidence = torch.divide(torch.randn(1, num_agents), 50).requires_grad_()\n",
    "listen_factor = torch.randn((num_agents, num_agents)).divide(10).fill_diagonal_(0).requires_grad_() # generate either 0 or 1\n",
    "\n",
    "real_evidence = 0 # the actual position of the evidence\n",
    "\n",
    "show_adjacency_matrix(listen_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0a46cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "for t in range(10000):\n",
    "    if t % 1000 == 999:\n",
    "        #real_evidence -= 0.02\n",
    "        display.clear_output(wait=True)\n",
    "        print(t, \"steps\")\n",
    "        plot_beliefs_and_evidence(beliefs, evidence)\n",
    "        show_adjacency_matrix(listen_factor)\n",
    "        print(\"Average agent utility:\", utility.mean().item())\n",
    "\n",
    "    #with torch.autograd.detect_anomaly():\n",
    "    self_utility = beliefs * evidence\n",
    "    scepticism_utility = scepticism/2*beliefs.square()\n",
    "    peer_utility = peer_multiplier * torch.sum((listen_factor * beliefs.t()), 0) * beliefs\n",
    "    evidence_scepticism_utility = evidence_pressure * (evidence - real_evidence).square()\n",
    "    utility =  self_utility - scepticism_utility + peer_utility - evidence_scepticism_utility\n",
    "    \n",
    "    # Find gradients for the beliefs to increase utility\n",
    "    utility.backward(torch.ones_like(utility))\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    with torch.no_grad():\n",
    "        beliefs += learning_rate * beliefs.grad\n",
    "        beliefs.grad.zero_()\n",
    "        beliefs = beliefs.clamp_(-1, 1)\n",
    "        \n",
    "        evidence += learning_rate * evidence.grad\n",
    "        evidence.grad.zero_()\n",
    "        evidence = evidence.clamp_(-1, 1)\n",
    "        \n",
    "        listen_factor += learning_rate * listen_factor.grad\n",
    "        listen_factor.grad.zero_()\n",
    "        listen_factor = listen_factor.fill_diagonal_(0)\n",
    "        listen_factor = listen_factor.clamp_(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f728d0",
   "metadata": {},
   "source": [
    "We find that agents end up forming their echochambers a lot faster when they are capable of changing their evidence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36916e45",
   "metadata": {},
   "source": [
    "# Next Episode\n",
    "\n",
    "In the next episode we will create models to see what ways there are of mitigating the formation of echochambers and how it could be possible to break agents out of them"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
