{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb9d37ac",
   "metadata": {},
   "source": [
    "# Echochambers Episode 1: The Optimization Menace\n",
    "Echochambers are the usually most vilifying and useless form of communities. Little critical thinking is invested into the ideas circulated beyond the justification of already held beliefs, and any conflicting ideas are usually ridiculed or just ignored at worst. Platforms like Twitter and Reddit provide safe haven for a many echochambers on the internet as they make it incredibly easy to become intrenched in certain communities (like subreddits or twitter sub-cultures).\n",
    "\n",
    "Here I attempt to model some of the behaviours of echochambers: simply at first, then building up complexity. I will see if I can find some non-intuitive and interesting conclusions while doing so.\n",
    "\n",
    "A lot of the initial modelling will come from Frank Witte's ECON0055 Economics of Science lectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc596b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9d25f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "num_agents = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a80ac2",
   "metadata": {},
   "source": [
    "Initially, we will model agents in a relatively simple way. Each of the ten agents will have a fixed evidence variable, which will be relatively small random value (0 mean and 0.1 SD). Then, they will have another attribute called their belief, which is another small random value (0 mean and 0.1 SD) that represents which way on a particular issue they believe in. In this simulation, the issue will just have two opposing viewpoints, and will be a continuous number from -inf to +inf representing which side and how strong they believe in the viewpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8b672d",
   "metadata": {},
   "outputs": [],
   "source": [
    "beliefs = torch.divide(torch.randn(1, num_agents), 10).requires_grad_()\n",
    "evidence = torch.divide(torch.randn(1, num_agents), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefcfc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(beliefs, evidence):\n",
    "    fig, axs = plt.subplots(2)\n",
    "    axs[0].imshow(beliefs.detach(), cmap=\"bwr\", norm=matplotlib.colors.Normalize(-1, 1, True))\n",
    "    axs[0].set_title(\"Agent beliefs\", fontsize=\"x-large\")\n",
    "    axs[0].set_xlabel(\"Agent number\")\n",
    "    axs[0].set_xticks(range(beliefs.shape[1]))\n",
    "    axs[0].tick_params(axis=\"y\", which=\"both\", left=False, right=False, labelleft=False)\n",
    "    axs[1].imshow(evidence.detach(), cmap=\"bwr\", norm=matplotlib.colors.Normalize(-1, 1, True))\n",
    "    axs[1].set_title(\"Agent evidence\", fontsize=\"x-large\")\n",
    "    axs[1].set_xlabel(\"Agent number\")\n",
    "    axs[1].set_xticks(range(evidence.shape[1]))\n",
    "    axs[1].tick_params(axis=\"y\", which=\"both\", left=False, right=False, labelleft=False)\n",
    "    plt.show()\n",
    "\n",
    "print(\"Initial beliefs and evidence:\")\n",
    "plot(beliefs, evidence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0260367d",
   "metadata": {},
   "source": [
    "Finally, we get them to maximize their utility. Their utility will be defined simply as the product of their beliefs and the evidence for that belief. It makes sense that for them to maximize their utility, they will have to change their beliefs to fit the little evidence that they have by making it the same sign. After each \"timeslot\", they will look at which way they should change their beliefs to increase their utility and make a small step towards it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee840d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(100000):\n",
    "\n",
    "    if t % 20000 == 19999:\n",
    "        display.clear_output(wait=True)\n",
    "        print(t, \"steps\")\n",
    "        plot(beliefs, evidence)\n",
    "        print(\"Average agent utility:\", utility.mean().item())\n",
    "\n",
    "    utility = beliefs * evidence\n",
    "    \n",
    "    # Find gradients for the beliefs to increase utility\n",
    "    utility.backward(torch.ones_like(utility))\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    with torch.no_grad():\n",
    "        beliefs += learning_rate * beliefs.grad\n",
    "        beliefs.grad.zero_()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ce1d63",
   "metadata": {},
   "source": [
    "As we can see, they extrapolate their beliefs no matter how little evidence that they have in order to increase their utility, becoming more and more \"radicalized\""
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
